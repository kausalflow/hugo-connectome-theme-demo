<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.68.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>人工神经网络方法解方程 | Connectome | KausalFlow</title><meta name=author content="KausalFlow"><meta property="og:title" content="人工神经网络方法解方程"><meta property="og:description" content="引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。"><meta property="og:type" content="article"><meta property="og:url" content="/posts/articles/2015-03-22-ann-ode/"><link rel=canonical href=/posts/articles/2015-03-22-ann-ode/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/lumen/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/logos/logo-square.png alt=Connectome height=28 style=margin-right:.5em> Connectome</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=/snm/ class=navbar-item>Spiking Neural Network</a>
<a href=/esl/ class=navbar-item>The Element of Statistical Learning</a>
<a href=/tutorials/ class=navbar-item>Tutorials</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/posts/>Posts</a>
<a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item target=blank href=https://github.com/kausalflow><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="人工神经网络方法解方程"><meta itemprop=description content="引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。"><meta itemprop=wordCount content="358"><meta itemprop=keywords content><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=/>Connectome</a></li><li><a href=/posts/>Blog Posts</a></li><li class=active><a href=/posts/articles/2015-03-22-ann-ode/>人工神经网络方法解方程</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">人工神经网络方法解方程</h1></header><div class=columns><div class="column is-8"><div class=is-divider data-content=SUMMARY></div><div class="notification is-light"><p>引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。</p></div><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><p>数值方法解方程大多要一个把方程或者函数参数化，例如改写成差分方程，变成步长等等的函数。例如我们要解这样一个方程：</p><p>$$\frac{dy(t)}{dt} = -y(t),$$</p><p>其中初始条件 $y(0)=1$ 已知。</p><p>一种普遍的手段是，我们可以把其中的待解函数参数化为 $y(t,\{n_i\})$，然后把方程重新写成</p><p>$$\frac{dy(t, {n_i})}{dt} + y(t,{n_i}) = 0,$$</p><p>其中 $\{n_i\}$ 是用来参数化这个函数的参数。有了这个式子，我们会联想到最小二乘法。也就是说，如果我们的参数化完全正确的话，我们应该得到方程的左边的两项之和总是零，也就是 $\frac{dy(t, \{n_i\})}{dt}$ 和 $- y(t,\{n_i\}) $ 的没有偏离，即</p><p>$$\text{waste} = \frac{dy(t, {n_i})}{dt} + y(t,{n_i}) ,$$</p><p>这个量总是零。然而数值上来讲，我们有一个更加简单的方法来让这个量在任意的 $t$ 总是零，方法就是借助最小二乘法的思路，利用一个凹函数，</p><p>$$\text{cost} = \int dt ( \frac{dy(t, {n_i})}{dt} + y(t,{n_i}) )^2.$$</p><p>倘若 $\text{cost} = 0$，我们推断上面定义的 $\text{waste}$ 函数在任意的 $t$ 都为零。</p><h2 id=人工神经网络参数化>人工神经网络参数化</h2><p>下面我们引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。</p><p>人的神经元有一个特点就是，存在激活和不激活两种状态，激活的话信号传递，不激活的话，信号被毙掉。数学上我们可以通过一个类似阶梯的函数来模拟这个行为，例如：</p><p>然而数值上来讲，这个函数有个非常大的缺点，就是导数不连续。尤其是对于我们想要解一个微分方程来说，用来参数化的函数里面包含导数不连续的部分，那是非常不方便的。</p><p>所以我们会选取一个导数连续但是也具有类似行为的函数，比如一个 sigmoid 函数，</p><p>$$\text{sigmoid}(t) = \frac{1}{1+e^{-t}}.$$</p><p>这个函数的图像是</p><p>这个函数有个非常大的优势，就是如果我们把要求解的函数利用这个参数化，所有的导数全是可以解析求解的，对于数值计算是一个福音。</p><p>我们可以模拟神经元可以被激活这种性质，来挑选出一些我们想要的参数，这个示例中用到的就是 sigmoid 函数。我们挑选一组参数 ${v_k}$, ${w_k}$ 和 ${u_k}$，对于一个函数 $y(t)$，我们可以用下面的方法来参数化，</p><p>$$y(t)= y(0)+t \sum_k v_k f(t w_k+u_k).$$</p><p>这样做的原因是在 $t=0$ 时，我们确保参数化的函数满足给定的初始条件（$t=0$ 时 $t \sum_k v_k f(t w_k+u_k)=0$）。而 $f(x)$ 使我们的启动函数，这里我们用 sigmoid 函数。也就是说，在某个时刻 $t_i$，当参量 $t w_k+u_k$ 比较大的时候，我们输入的 $t$ 才会起作用，否则我们的输入 $t$ 被压制。</p><p>$w_k$ 是对启动函数的缩放，$u_k$ 是对启动函数的平移，而 $v_k$ 的作用是放大缩小函数值。可以设想，当我们的参数足够多的时候，我们可以用这个参数化来组成任何连续函数。</p><h2 id=cost>cost</h2><p>那么如何得到这些参数的值呢？我们一开始定义了这样一个量，</p><p>$$\text{cost} = \int dt ( \frac{dy(t, {n_i})}{dt} + y(t,{n_i}) )^2.$$</p><p>我们想要做的是得到什么的参数可以有 $\text{cost} = 0$ 的结果。</p><p>然而我们不想做积分，所以自然的选择是离散化，选择一个序列 ${t_i}$，并且考虑</p><p>$$\text{cost} = \sum_j \left(\frac{dy(t_j, {n_i})}{dt} + y(t_j,{n_i}) \right)^2.$$</p><p>当我们选到一组参数使得这个量为零的时候，我们的参数就是函数的正确的参数化（在当前的初始条件下的）。</p><p>所以我们就把一个解方程问题转换成了一个最小化问题了，因为 $\text{cost}$ 的值越小，说明我们的参数就越接近满足</p><p>$$\frac{dy(t, {n_i})}{dt} + y(t,{n_i}) = 0,$$</p><p>当然是在一个特定初始条件下，这里我们的初始条件是 $y(0)=1$。</p><p>剩下的部分，就无需我多说了，只需要用任何你想要的方法，来把 $\text{cost}$ 最小化，得到的参数值们 $\{v_k\}$, $\{w_k\}$ 和 $\{u_k\}$ 带回到函数的参数化</p><p>$$y(t)= y(0)+t \sum_k v_k f(t w_k+u_k),$$</p><p>这样我们就有了这个微分方程在当前初始条件下的解。</p><p>上面我们离散化 cost 的过程中，</p><p>$$\text{cost} = \sum_j \frac{dy(t_j, {n_i})}{dt} + y(t_j,{n_i}) )^2.$$</p><p>里面的每一个时间点的输入都是一次对神经网络的训练，如同人的学习一样，经过多次训练，我们就可以掌握方程的这种行为。而所学习到的信息，存在了神经网络的参数中（也就是网络的结构中）。</p><h2 id=代码举例>代码举例</h2><p>以下是这个问题的 Python 代码，<a href=http://nbviewer.ipython.org/github/NeuPhysics/sync-de-solver/blob/master/ipynb/neural-net.ipynb>这里有一份带有全面说明的 IPython Notebook</a>。</p><p>{% highlight python %}
import numpy as np
from scipy.optimize import minimize
from scipy.special import expit
import matplotlib.pyplot as plt</p><p>def cost(v,w,u,t):
v = np.array(v) # Don&rsquo;t know why but np.asarray(v) doesn&rsquo;t work here.
w = np.array(w)
u = np.array(u)</p><pre><code>fvec = np.array(trigf(t*w + u) )  # This is a vector!!!
yt = 1 + np.sum ( t * v * fvec  )  # For a given t, this calculates the value of y(t), given the parameters, v, w, u.
return  ( np.sum (v*fvec + t * v* fvec * ( 1 -  fvec  ) * w ) + yt )   ** 2
</code></pre><p>def trigf(x):
#return 1/(1+np.exp(-x)) #
return expit(x)</p><p>def costTotal(v,w,u,t):
t = np.array(t)
costt = 0
for temp in t:
costt = costt + cost(v,w,u,temp)
return costt</p><p>costTotalF = lambda x: costTotal(np.split(x,3)[0],np.split(x,3)[1],np.split(x,3)[2],tlin)</p><p>initGuess = np.zeros(30)
result = minimize(costTotalF,initGuess,method="SLSQP&rdquo;)
{% endhighlight %}</p><h2 id=为什么找这么多麻烦>为什么找这么多麻烦</h2><p>显然，上面的例子不仅可以用任何差分方法来解，而且有解析解。我们为什么要找这么多麻烦来做这样的参数化呢？</p><p>或许我们要解决一个当前的状态依赖于全空间中的所有点的问题，这时候，我们就不好再用差分法了，因为要解出当前点我们需要知道所有的其他的点。上面这种一次性解出整个方程的方法就变得更加方便，因为我们解出方程并不是依赖于获得其他地方的信息，而是通过人工神经网络一次性猜到所有空间点的解。</p><p>不过，本文开始讲的这种方法具有非常好的普适性。我们使用 ANN 是因为 Cybenko 在 1989 年证明了一个 sigmoid 可以作为很好的 universal approximator 来处理任意的 measurable functions。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>现在我们重新思考一下这个方法的普适性。倘若我们有个系统，可以用傅里叶展开，而且所幸我们只需要展开中的前 100 项就可以很好的来估算整个系统了。那么我们就把这个展开式写出来，然后取前 100 项，这样一样定义一个 cost，一样来找到使得 cost 最小化的参数，就可以求出傅里叶展开的系数们了。</p><p>不过这个方法是不是足够有效，取决于我们所挑选的参数化形式。Kolmogorov 证明过如果选的足够好，我们可以用<strong>有限个</strong>与 y 无关的函数来精确的还原 y。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=http://numsoltun.readthedocs.org/ann.html#universal-approximators>Click here to find out the statement.</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time>
by <span itemprop=author>KausalFlow</span>;</div></p></div><div class="column is-4"><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#人工神经网络参数化>人工神经网络参数化</a></li><li><a href=#cost>cost</a></li><li><a href=#代码举例>代码举例</a></li><li><a href=#为什么找这么多麻烦>为什么找这么多麻烦</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content="Cite Me"></div><div class="box is-size-7 has-text-white has-background-black"><article class=media><div class=media-content><div class=content><p>OctoMiao (0001). '人工神经网络方法解方程', Connectome, 01 April. Available at: /posts/articles/2015-03-22-ann-ode/.</p></div></div></article></div></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/kausalflow/hugo-connectome-theme-demo/edit/master/content/posts/articles/2015-03-22-ann-ode.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a><a class="button is-primary is-light is-outlined" data-lyket-type=updown data-lyket-id=2015-03-22-ann-ode data-lyket-namespace=Connectome data-lyket-template=reddit style=width:35px;height:35px;position:fixed;bottom:120px;right:10px;border-radius:9999px></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=/>KausalFlow</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script src="https://unpkg.com/@lyket/widget@latest/dist/lyket.js?apiKey=4df20b7e32f469fed5dc53f5ab39d8&disableSessionId"></script><script async type=text/javascript src=/js/bulma.js></script></body></html>